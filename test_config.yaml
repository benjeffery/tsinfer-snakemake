# File locations - note the {chrom} interpolations, which will be filled by
# the values from regions below

ancestral_fasta: test_data/truncated.fasta
vcf: test_data/vcf{chrom}.gz
recomb_map: test_data/genetic_map_Hg38_chr{chrom}.txt

# https://www.biostars.org/p/435003/#462800
regions:
#  chr1p: 1:1-123400000
#  chr1q: 1:123400001-248924793
#  chr2p: 2:1-93900000
#  chr2q: 2:93900001-242148846
#  chr3p: 3:1-90900000
#  chr3q: 3:90900001-198147657
#  chr4p: 4:1-50000000
#  chr4q: 4:50000001-190107490
#  chr5p: 5:1-48800000
#  chr5q: 5:48800001-181288809
#  chr6p: 6:1-59800000
#  chr6q: 6:59800001-170742228
#  chr7p: 7:1-60100000
#  chr7q: 7:60100001-159334084
#  chr8p: 8:1-45200000
#  chr8q: 8:45200001-145075770
#  chr9p: 9:1-43000000
#  chr9q: 9:43000001-138219529
#  chr10p: 10:1-39800000
#  chr10q: 10:39800001-133766368
#  chr11p: 11:1-53400000
#  chr11q: 11:53400001-135075628
#  chr12p: 12:1-35500000
#  chr12q: 12:35500001-133202210
#  chr13p: 13:1-17700000
#  chr13q: 13:17700001-114341521
#  chr14p: 14:1-17200000
#  chr14q: 14:17200001-106880170
#  chr15p: 15:1-19000000
#  chr15q: 15:19000001-101965905
#  chr16p: 16:1-36800000
#  chr16q: 16:36800001-90097180
#  chr17p: 17:1-25100000
#  chr17q: 17:25100001-83087393
#  chr18p: 18:1-18500000
#  chr18q: 18:18500001-80257297
#  chr19p: 19:1-26200000
#  chr19q: 19:26200001-58585793
#  chr20p: 20:1-28100000
#  chr20q: 20:28100001-64318092
#  chr21p: 21:1-12000000
#  chr21q: 21:12000001-46680243
  chr22p: 22:1-15000000
#  chr22q: 22:15000001-50791377


# A list of the mismatch values to use, note that a non-zero mismatch value
# results in about 5x slower runtimes.
mismatch_values: [0, 1.0e-9]

# A list of truncation options to run, note that
# all zeros disables truncation
truncate:
  - lower: 0
    upper: 0
    multiplier: 0
  - lower: 0.2
    upper: 0.8
    multiplier: 1

# Pre-tsinfer filters. Each entry is a named filter set that will be run.
# Any of the filters defined in filters.py can be referenced here.
filters:
  filter_minimal:
    duplicate_position:
  filter_nton23:
    duplicate_position:
    not_snps:
    not_biallelic:
    bad_ancestral:
    no_ancestral_allele:
    low_quality_ancestral_allele:
    low_allele_count:
        # Note that any change to these params may not be detected by snakemake
        min_derived: 2
        min_ancestral: 3

vcf_to_zarr:
  # These parameters influence the peak RAM usage of the vcf_to_zarr steps
  # For a ~100,000 samples VCF these settings result in about 16GB peak RAM usage
  # per core. If you have less RAM or more samples you may need to reduce the
  # values here, keeping chunk_length > temp_chunk_length > read_chunk_length.
  read_chunk_length: 10000
  temp_chunk_length: 20000
  chunk_length: 40000
  chunk_width: 1500
  # Temp files are deleted serially, so with large VCFs on slow NFS mounts this
  # can take a long time. If you have a fast filesystem you can set this to False
  retain_temp_files: True

# Dask is optional for the following steps, if you're on a single machine set these
# to False
match_ancestors:
  use_dask: True
match_samples:
  use_dask: True

# Directory to keep output, progress and resume files
data_dir: "test_data/workflow"

# Directory to keep temporary files
temp_dir: "test_data/workflow/temp"

# A dict of subsets to run
# Key names will be used in the output file names, the referenced file should
# be a CSV with a single, header-less column containing the sample IDs to include.
sample_subsets:
  subset: "test_data/subset_samples.csv"

# Place dask config here
dask:
  distributed.scheduler.allowed-failures: 10
  distributed.scheduler.worker-saturation: 1.1

# Cluster config used by the "snakemake-with-dask" script
cluster:
#  class: dask_jobqueue.LSFCluster
#  kwargs:
#      queue: "short"
#      project: "gecip_lsf_access"
#      cores: 1
#      walltime: "03:59"
#      memory: "15GB"
#      use_stdin: True
#      lsf_units: "MB"
#      job_script_prolouge: ['#BSUB -R "rusage[mem=16000] span[hosts=1]"']
#      worker_extra_args:
#        - "--lifetime"
#        - "230m"
#        - "--lifetime-stagger"
#        - "5m"
#  adapt:
#    maximum_jobs: 750
#    minimum_jobs: 200
#    wait_count: 10
  class: dask.distributed.LocalCluster

# Alternatively, specify a dask cluster directly
# scheduler_address: "tcp://x.x.x.x:xxxxx"

# Max threads that will be submitted to the cluster/local
max_threads: 8

# Max mem to ask the cluster for
max_mem: 750GB

# Max time to ask the cluster for in minutes
max_time: 262800 # 6 months

custom_resources:
  # Each snakemake task that uses the dask cluster can run while other tasks
  # are using the same cluster. In order to not overload the cluster
  # we assign a usage in arbitrary units. With the max being 10.
  dask_cluster: 10

prefix: ""
#prefix: "module load python/3.8.2 && source tsinfer-env/bin/activate"
