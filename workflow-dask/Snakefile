import os.path
import dask
from pathlib import Path
from dask.distributed import Client, performance_report

configfile: "config.yaml"
shell.prefix(config["prefix"])

for k,v in config["dask"].items():
    dask.config.set({k:v})

data_dir=Path(config['data_dir'])

# These are quick steps that we don't want to submit jobs for
localrules: all, summary_table

def parse_region(region):
    chrom, rest = region.split(":")
    start, stop = map(int, rest.split("-"))
    return chrom, start, stop

rule all:
    input:
        expand(data_dir/"{subset_name}-region_summary_table.csv", subset_name=config["sample_subsets"].keys()),
        expand(data_dir/ "trees" / "{subset_name}-{region_name}" / "{subset_name}-{region_name}-mm{mismatch}.trees",
            subset_name=config["sample_subsets"].keys(),
            region_name=config["regions"].keys(),
            mismatch=config["mismatch_values"],
        )


rule vcf_to_zarrs:
    input:
        vcf=lambda wildcards: config["vcf"].format(chrom=wildcards.chrom_num),
        tbi=lambda wildcards: config["vcf"].format(chrom=wildcards.chrom_num)+".tbi"
    output:
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/".vcf_done",
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"performance_report.html"
    resources:
        dask_cluster=10,
        mem_mb=16000,
        time_min=239,
        runtime=239
    params:
        target_part_size="5M",
        read_chunk_length=10_000,
        temp_chunk_length=20_000,
        chunk_length=40_000,
        chunk_width=1_500,
        retain_temp_files=True
    run:
        import sgkit.io.vcf
        client = Client(config["scheduler_address"])
        with performance_report(
            filename=output[1]
        ):
            sgkit.io.vcf.vcf_reader.vcf_to_zarr(
                input[0],
                output[0].replace(".vcf_done", ""),
                target_part_size=params.target_part_size,
                read_chunk_length=params.read_chunk_length,
                temp_chunk_length=params.temp_chunk_length,
                chunk_length=params.chunk_length,
                chunk_width=params.chunk_width,
                tempdir=config["temp_dir"],
                retain_temp_files=params.retain_temp_files
            )
        Path(str(output[0])).touch()

rule load_ancestral_fasta:
    input:
        lambda wildcards: config["ancestral_fasta"].format(chrom=wildcards.chrom_num),
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/".vcf_done"
    output:
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_ancestral_allele")
    threads: 1
    resources:
        mem_mb=16000,
        time_min=30,
        runtime=30
    run:
        import pyfaidx
        import numpy
        import sgkit
        import xarray
        fasta = pyfaidx.Fasta(input[0])
        seq_name = list(fasta.keys())[0]
        print("Ancestral sequence:", seq_name)
        ancestral_sequence = numpy.asarray(fasta[seq_name], dtype="U1")
        # From the human ancestral states README:
        # The convention for the sequence is:
        #    ACTG : high-confidence call, ancestral state supported by other 2 sequences
        #    actg : low-confidence call, ancestral state supported by one sequence only
        #    N    : failure, the ancestral state is not supported by any other sequence
        #    -    : the extant species contains an insertion at this position
        #    .    : no coverage in the alignment
        ds_dir = input[1].replace(".vcf_done", "")
        ds = sgkit.load_dataset(ds_dir)
        ancestral_states = numpy.char.upper(ancestral_sequence[ds['variant_position'].values-1])
        ancestral_states = xarray.DataArray(data=ancestral_states, dims=["variants"], name="variant_ancestral_allele")
        ds.update({"variant_ancestral_allele": ancestral_states})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_ancestral_allele"}), ds_dir, mode="a")

rule mask_sites:
    input:
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/".vcf_done",
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_ancestral_allele"
    output:
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_mask"),
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_duplicate_position_mask"),
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_bad_ancestral_mask"),
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_no_ancestral_allele_mask")
    resources:
        mem_mb=16000,
        time_min=30,
        runtime=30
    run:
        #No need for a dask cluster here
        from xarray import full_like
        import numpy
        import sgkit
        ds_dir = input[0].replace(".vcf_done", "")
        ds = sgkit.load_dataset(ds_dir)
        chunks = ds.variant_position.chunks

        # Mask bad ancestral sites
        wanted_variants = (ds['variant_ancestral_allele'] != '-') & (ds['variant_ancestral_allele'] != '.') & (ds['variant_ancestral_allele'] != 'N')
        wanted_variants = wanted_variants.rename("variant_bad_ancestral_mask").chunk(chunks).compute()
        assert set(numpy.unique(ds['variant_ancestral_allele'][wanted_variants])) == {'A', 'C', 'G', 'T'}
        ds.update({"variant_bad_ancestral_mask": wanted_variants})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_bad_ancestral_mask"}), ds_dir, mode="a")

        # Mask sites where the ancestral state is not an allele
        wanted_variants = ((ds['variant_ancestral_allele'] == ds['variant_allele'][:,0]) |
                           (ds['variant_ancestral_allele'] == ds['variant_allele'][:,1]))
        wanted_variants = wanted_variants.rename("variant_no_ancestral_allele_mask").chunk(chunks).compute()
        ds.update({"variant_no_ancestral_allele_mask": wanted_variants})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_no_ancestral_allele_mask"}), ds_dir, mode="a")

        # Mask duplicate sites with duplicate position
        pos = ds['variant_position']
        pos_shift_left = full_like(pos,-1)
        pos_shift_left[0:-1] = pos[1:]
        pos_shift_right = full_like(pos,-1)
        pos_shift_right[1:] = pos[:-1]
        wanted_variants = (pos != pos_shift_left) & (pos != pos_shift_right)
        wanted_variants = wanted_variants.rename("variant_duplicate_position_mask").chunk(chunks).compute()
        ds.update({"variant_duplicate_position_mask": wanted_variants})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_duplicate_position_mask"}), ds_dir, mode="a")

        ## Create the combined mask - tsinfer now copes with missing and bad ancestral sites so don't use those masks
        wanted_variants = ds['variant_duplicate_position_mask']
        wanted_variants = wanted_variants.rename("variant_mask").chunk(chunks).compute()
        ds.update({"variant_mask": wanted_variants})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_mask"}), ds_dir, mode="a")

rule subset_zarr_vcf:
    input:
        lambda wildcards: [(data_dir/"zarr_vcfs"/f"chr{parse_region(config['regions'][wildcards.region_name])[0]}"/"data.zarr"/suffix)
           for suffix in [".vcf_done", "variant_ancestral_allele", "variant_mask",
                          "variant_duplicate_position_mask", "variant_bad_ancestral_mask",
                          "variant_no_ancestral_allele_mask"]
        ],
        lambda wildcards: config['sample_subsets'][wildcards.subset_name],
    output:
        directory(data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr")
    resources:
        dask_cluster=5,
        mem_mb=16000,
        time_min=239,
        runtime=239
    run:
        import numpy
        import sgkit
        client = Client(config["scheduler_address"])
        chrom, start, end = parse_region(config['regions'][wildcards.region_name])
        ds = sgkit.load_dataset(input[0].replace(".vcf_done", ""))
        with open(input[-1],'r') as f:
            sample_ids = numpy.genfromtxt(f,dtype=str)
        sample_mask = numpy.isin(ds.sample_id.values, sample_ids)
        variant_position = ds['variant_position'].values
        variant_mask = numpy.logical_and(variant_position >= start, variant_position < end)
        ds = ds.sel(samples=sample_mask, variants=variant_mask)
        ds = ds.unify_chunks()
        sgkit.save_dataset(ds,output[0].replace(".vcf_done", ""), auto_rechunk=True)
        Path(str(output[0])).touch()

rule zarr_stats:
    input:
        data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr"
    output:
        data_dir/"zarr_stats"/"{subset_name}-{region_name}"/"stats.json"
    resources:
        dask_cluster=5,
        mem_mb=16000,
        time_min=30,
        runtime=30
    run:
        import sgkit
        import json
        client = Client(config["scheduler_address"])
        ds = sgkit.load_dataset(input[0].replace(".vcf_done", ""))
        out = {}
        out['dataset_summary'] = str(ds)
        out['name'] = wildcards.region_name
        out['n_samples'] = ds.dims['samples']
        out['n_variants'] = ds.dims['variants']
        out['n_ploidy'] = ds.dims['ploidy']
        #Flatten the ploidy dimension as tsinfer sees each phased haplotype as a sample
        gt = (ds.call_genotype.stack(haplotype=("samples","ploidy")))
        out['allele_counts'] = (gt > 0).sum(dim=['haplotype']).to_numpy().tolist()
        out['missing_count'] = int((gt == -1).sum())
        out['num_sites_triallelic'] = int(((gt > 1).sum(dim=["haplotype"]) > 0).sum())
        out['sites_bad_ancestral'] = int((~ds.variant_bad_ancestral_mask).sum())
        out['sites_no_ancestral'] = int((~ds.variant_no_ancestral_allele_mask).sum())
        out['sites_duplicate_pos'] = int((~ds.variant_duplicate_position_mask).sum())
        out['sites_masked'] = int((~ds.variant_mask).sum())
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(input[0].replace(".vcf_done", "")):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                total_size += os.path.getsize(fp)
        out['size'] = total_size
        with open(output[0], "w") as f:
            f.write(json.dumps(out))


checkpoint summary_table:
    input:
        lambda wildcards: [data_dir/"zarr_stats"/f"{wildcards.subset_name}-{region_name}"/"stats.json" for region_name in config["regions"].keys()]
    output:
        data_dir/"{subset_name}-region_summary_table.csv"
    run:
        import csv
        import json
        def number_to_SI(number):
            """Convert a number to a string with SI units, unless it is a string already."""
            units = ["", "K", "M", "G", "T", "P", "E", "Z", "Y"]
            unit = 0
            if isinstance(number, str):
                return number
            while number > 1000:
                number /= 1000
                unit += 1
            return f"{number:.2f}{units[unit]}"

        header = ["region_name", "vcf_size", "zarr_size", "n_variants", "n_samples", "ac==0", "ac==1",
        "ac==2", "missing_genotypes", "num_sites_triallelic", "sites_bad_ancestral", "sites_no_ancestral",
        "sites_duplicate_pos", "sites_masked", "inference_nbytes", "inference_bitpack_nbytes"]
        with open(output[0], "w", newline='') as f:
            writer = csv.DictWriter(f, fieldnames=header)
            writer.writeheader()

            for vcf_stats in input:
                with open(vcf_stats, "r") as json_stats_f:
                    stats = json.load(json_stats_f)
                    ac0 = sum([ac == 0 for ac in stats['allele_counts']])
                    ac1 = sum([ac == 1 for ac in stats['allele_counts']])
                    ac2 = sum([ac == 2 for ac in stats['allele_counts']])
                    n_sites = stats['n_variants']
                    n_samples = stats['n_samples']
                    n_ploidy = stats['n_ploidy']
                    n_masked = stats['sites_masked']
                    chrom, start, stop = parse_region(config["regions"][stats['name']])
                    row_dict = {
                        "region_name": stats['name'],
                        "vcf_size": os.path.getsize(config['vcf'].format(chrom=chrom)),
                        "zarr_size": stats['size'],
                        "n_variants": n_sites,
                        "n_samples": n_samples,
                        "ac==0": ac0,
                        "ac==1": ac1,
                        "ac==2": ac2,
                        "missing_genotypes": stats['missing_count'],
                        "num_sites_triallelic": stats['num_sites_triallelic'],
                        "sites_bad_ancestral": stats['sites_bad_ancestral'],
                        "sites_no_ancestral": stats['sites_no_ancestral'],
                        "sites_duplicate_pos": stats['sites_duplicate_pos'],
                        "sites_masked": n_masked,
                        "inference_nbytes": ((n_sites-n_masked) - ac1) * n_samples * n_ploidy,
                        "inference_bitpack_nbytes": (((n_sites-n_masked) - ac1) * n_samples * n_ploidy) / 8
                    }
                    row_dict = {k: number_to_SI(v) for k, v in row_dict.items()}
                    writer.writerow(row_dict)

def get_ancestor_gen_memory(wildcards):
    import numpy
    import json
    #Use the checkpoint to check the stats file exists
    checkpoint_output = checkpoints.summary_table.get(subset_name=wildcards.subset_name)
    region_stats = data_dir / "zarr_stats" / f"{wildcards.subset_name}-{wildcards.region_name}" / "stats.json"
    with open(region_stats, "r") as json_stats_f:
        with open(config['sample_subsets'][wildcards.subset_name],'r') as f:
            n_samples = len(numpy.genfromtxt(f,dtype=str))
        stats = json.load(json_stats_f)
        n_ploidy = stats['n_ploidy']
        n_sites = stats['n_variants']
        n_masked = stats['sites_masked']
        ac1 = sum([ac == 1 for ac in stats['allele_counts']])
        mem = int(5_000 + (((n_sites-n_masked) - ac1) * n_samples * n_ploidy) / (8 * 1_048_576))
        return mem

def get_ancestor_cpus(wildcards):
    import math
    #Use the checkpoint to check the stats file exists
    checkpoint_output = checkpoints.summary_table.get(subset_name=wildcards.subset_name)
    #For the GEL cluster, at least, we must ask for more CPUs to get more memory
    return max(4, math.ceil(get_ancestor_gen_memory(wildcards) / 16_000))

rule generate_ancestors:
    input:
        data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr",
        data_dir/"zarr_stats"/"{subset_name}-{region_name}"/"stats.json"
    output:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors.zarr"
    threads: 96#get_ancestor_cpus
    resources:
        mem_mb=750_000,#get_ancestor_gen_memory,
        time_min=14 * 24 * 60,
        runtime=14 * 24 * 60
    run:
        import tsinfer
        import logging
        logging.basicConfig(level=logging.INFO)
        sample_data = tsinfer.SgkitSampleData(input[0])
        #Limit the threads to 8 as it too many leads to contention
        os.makedirs(data_dir/"progress"/"generate_ancestors", exist_ok=True)
        with open(data_dir/"progress"/"generate_ancestors"/f"{wildcards.subset_name}-{wildcards.region_name}.log", "w") as log_f:
             tsinfer.generate_ancestors(
                 sample_data,
                 path=output[0],
                 genotype_encoding=1,
                 num_threads=min(threads,8),
                 progress_monitor=tsinfer.progress.ProgressMonitor(tqdm_kwargs={'file':log_f, 'mininterval':30})
             )

rule truncate_ancestors:
    input:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors.zarr"
    output:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors-truncated.zarr"
    threads: 1
    resources:
        mem_mb=64_000,
        time_min=24 * 60,
        runtime=24 * 60
    run:
        import tsinfer
        import logging
        logging.basicConfig(level=logging.INFO)
        ancestors = tsinfer.AncestorData.load(input[0])
        truncated = ancestors.truncate_ancestors(0.4, 0.6, length_multiplier=1, path=output[0])

rule match_ancestors:
    input:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors-truncated.zarr",
        data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr",
    output:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors.trees",
    threads: 96
    resources:
        mem_mb = 750_000,
        time_min=14 * 24 * 60,
        runtime=14 * 24 * 60
    run:
        import tsinfer
        import logging
        import os
        logging.basicConfig(level=logging.INFO)
        ancestors = tsinfer.AncestorData.load(input[0])
        sample_data = tsinfer.SgkitSampleData(input[1])
        os.makedirs(data_dir/"progress"/"match_ancestors", exist_ok=True)
        os.makedirs(data_dir/"resume"/"match_ancestors", exist_ok=True)
        os.makedirs(os.path.dirname(output[0]), exist_ok=True)
        with open(data_dir/"progress"/"match_ancestors"/f"{wildcards.subset_name}-{wildcards.region_name}.log", "w") as log_f:
            ts = tsinfer.match_ancestors(
                sample_data,
                ancestors,
                path_compression=True,
                num_threads=threads,
                precision=15,
                progress_monitor=tsinfer.progress.ProgressMonitor(tqdm_kwargs={'file':log_f, 'mininterval':30}),
                resume_lmdb_file=str(data_dir/"resume"/"match_ancestors"/f"{wildcards.subset_name}-{wildcards.region_name}.lmdb"),
                use_dask=False,
            )
        ts.dump(output[0])

rule match_samples:
    input:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors.trees",
        data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr",
        lambda wildcards: config['recomb_map'].format(chrom=parse_region(config['regions'][wildcards.region_name])[0])
    output:
        data_dir/"trees"/"{subset_name}-{region_name}"/"{subset_name}-{region_name}-mm{mismatch}.trees",

    #Minimal threads as we're using dask
    threads: 96
    resources:
        mem_mb = 750_000,
        time_min=14 * 24 * 60,
        runtime=14 * 24 * 60
    run:
        import tsinfer
        import logging
        import tskit
        import numpy
        import msprime

        logging.basicConfig(level=logging.INFO)
        anc_ts = tskit.load(input[0])
        sample_data = tsinfer.SgkitSampleData(input[1])
        os.makedirs(data_dir/"progress"/"match_samples", exist_ok=True)
        os.makedirs(data_dir /"resume"/"match_samples",exist_ok=True)
        os.makedirs(os.path.dirname(output[0]),exist_ok=True)
        mismatch = float(wildcards.mismatch)
        if mismatch > 0:
            inference_pos = anc_ts.tables.sites.position
            rate_map = msprime.RateMap.read_hapmap(
                input[-1],
                position_col=1,
                rate_col=2
            )
            genetic_dists = tsinfer.Matcher.recombination_rate_to_dist(
                rate_map,inference_pos
            )
            recombination_map = tsinfer.Matcher.recombination_dist_to_prob(genetic_dists)
            #Set 0 probabilities to a small value
            recombination_map[recombination_map == 0] = 1e-19
            mismatch_ratio = mismatch
            num_alleles = 2
            mismatch_map = numpy.full(
                len(inference_pos),
                tsinfer.Matcher.mismatch_ratio_to_prob(
                    mismatch_ratio,
                    numpy.median(genetic_dists),
                    num_alleles
                )
            )
        else:
            recombination_map = None
            mismatch_map = None
        with open(data_dir/"progress"/"match_samples"/f"{wildcards.subset_name}-{wildcards.region_name}-{wildcards.mismatch}.log", "w") as log_f:
            ts = tsinfer.match_samples(
                sample_data,
                anc_ts,
                path_compression=True,
                num_threads=threads,
                recombination=recombination_map,
                mismatch=mismatch_map,
                precision=15,
                progress_monitor=tsinfer.progress.ProgressMonitor(tqdm_kwargs={'file':log_f, 'mininterval':30}),
                resume_lmdb_file = str(data_dir / "resume" / "match_samples" / f"{wildcards.subset_name}-{wildcards.region_name}-{wildcards.mismatch}.lmdb"),
                use_dask=True,
            )
        ts.dump(output[0])
